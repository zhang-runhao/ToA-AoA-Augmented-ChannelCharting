{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhang-runhao/ToA-AoA-Augmented-ChannelCharting/blob/main/056_DownloadDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAJ_GU7z_98J",
        "outputId": "c1dd3696-eadb-4f60-99dd-4db18197b23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-05 10:13:32--  https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/8\n",
            "Resolving darus.uni-stuttgart.de (darus.uni-stuttgart.de)... 129.69.7.87, 2001:7c0:2041:17f::87\n",
            "Connecting to darus.uni-stuttgart.de (darus.uni-stuttgart.de)|129.69.7.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e4e178e-eb04ebca7449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf02.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=af1c63565dbfaae1b8914af890a0ef04fd94c486f2f5dfc705411f13a34a34f3 [following]\n",
            "--2024-02-05 10:13:32--  https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e4e178e-eb04ebca7449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf02.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=af1c63565dbfaae1b8914af890a0ef04fd94c486f2f5dfc705411f13a34a34f3\n",
            "Resolving s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)... 129.69.5.100, 129.69.5.99, 2001:7c0:2041:1db::99, ...\n",
            "Connecting to s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)|129.69.5.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4862708952 (4.5G) [application/octet-stream]\n",
            "Saving to: ‘dataset/dichasus-cf02.tfrecords’\n",
            "\n",
            "dichasus-cf02.tfrec 100%[===================>]   4.53G  37.8MB/s    in 2m 39s  \n",
            "\n",
            "2024-02-05 10:16:11 (29.2 MB/s) - ‘dataset/dichasus-cf02.tfrecords’ saved [4862708952/4862708952]\n",
            "\n",
            "--2024-02-05 10:16:11--  https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/9\n",
            "Resolving darus.uni-stuttgart.de (darus.uni-stuttgart.de)... 129.69.7.87, 2001:7c0:2041:17f::87\n",
            "Connecting to darus.uni-stuttgart.de (darus.uni-stuttgart.de)|129.69.7.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e4ffaa7-444e1e441a38?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf03.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101611Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=3a13ccb529e1fbb79aa7e4a60ce295fea0f4f21c4189a6e3885f4aa2b9fdf02d [following]\n",
            "--2024-02-05 10:16:11--  https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e4ffaa7-444e1e441a38?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf03.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101611Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=3a13ccb529e1fbb79aa7e4a60ce295fea0f4f21c4189a6e3885f4aa2b9fdf02d\n",
            "Resolving s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)... 129.69.5.99, 129.69.5.100, 2001:7c0:2041:1db::100, ...\n",
            "Connecting to s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)|129.69.5.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6165839316 (5.7G) [application/octet-stream]\n",
            "Saving to: ‘dataset/dichasus-cf03.tfrecords’\n",
            "\n",
            "dichasus-cf03.tfrec 100%[===================>]   5.74G  36.4MB/s    in 3m 7s   \n",
            "\n",
            "2024-02-05 10:19:19 (31.4 MB/s) - ‘dataset/dichasus-cf03.tfrecords’ saved [6165839316/6165839316]\n",
            "\n",
            "--2024-02-05 10:19:19--  https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/10\n",
            "Resolving darus.uni-stuttgart.de (darus.uni-stuttgart.de)... 129.69.7.87, 2001:7c0:2041:17f::87\n",
            "Connecting to darus.uni-stuttgart.de (darus.uni-stuttgart.de)|129.69.7.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e532723-5913ded52ae6?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf04.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101919Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=341320bc7ed733d0fb6b49292facff7ad9c97b74eb0799b847213b9e870d9130 [following]\n",
            "--2024-02-05 10:19:19--  https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811e532723-5913ded52ae6?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27dichasus-cf04.tfrecords&response-content-type=application%2Foctet-stream&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T101919Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=341320bc7ed733d0fb6b49292facff7ad9c97b74eb0799b847213b9e870d9130\n",
            "Resolving s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)... 129.69.5.100, 129.69.5.99, 2001:7c0:2041:1db::100, ...\n",
            "Connecting to s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)|129.69.5.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11027760402 (10G) [application/octet-stream]\n",
            "Saving to: ‘dataset/dichasus-cf04.tfrecords’\n",
            "\n",
            "dichasus-cf04.tfrec 100%[===================>]  10.27G  34.0MB/s    in 5m 33s  \n",
            "\n",
            "2024-02-05 10:24:53 (31.6 MB/s) - ‘dataset/dichasus-cf04.tfrecords’ saved [11027760402/11027760402]\n",
            "\n",
            "--2024-02-05 10:24:53--  https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf02.json\n",
            "Resolving dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)... 129.69.175.114\n",
            "Connecting to dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)|129.69.175.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1836 (1.8K) [application/json]\n",
            "Saving to: ‘dataset/reftx-offsets-dichasus-cf02.json’\n",
            "\n",
            "reftx-offsets-dicha 100%[===================>]   1.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-05 10:24:53 (256 MB/s) - ‘dataset/reftx-offsets-dichasus-cf02.json’ saved [1836/1836]\n",
            "\n",
            "--2024-02-05 10:24:53--  https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf03.json\n",
            "Resolving dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)... 129.69.175.114\n",
            "Connecting to dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)|129.69.175.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1830 (1.8K) [application/json]\n",
            "Saving to: ‘dataset/reftx-offsets-dichasus-cf03.json’\n",
            "\n",
            "reftx-offsets-dicha 100%[===================>]   1.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-05 10:24:53 (247 MB/s) - ‘dataset/reftx-offsets-dichasus-cf03.json’ saved [1830/1830]\n",
            "\n",
            "--2024-02-05 10:24:53--  https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf04.json\n",
            "Resolving dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)... 129.69.175.114\n",
            "Connecting to dichasus.inue.uni-stuttgart.de (dichasus.inue.uni-stuttgart.de)|129.69.175.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1825 (1.8K) [application/json]\n",
            "Saving to: ‘dataset/reftx-offsets-dichasus-cf04.json’\n",
            "\n",
            "reftx-offsets-dicha 100%[===================>]   1.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-05 10:24:53 (246 MB/s) - ‘dataset/reftx-offsets-dichasus-cf04.json’ saved [1825/1825]\n",
            "\n",
            "--2024-02-05 10:24:53--  https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/1\n",
            "Resolving darus.uni-stuttgart.de (darus.uni-stuttgart.de)... 129.69.7.87, 2001:7c0:2041:17f::87\n",
            "Connecting to darus.uni-stuttgart.de (darus.uni-stuttgart.de)|129.69.7.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811417c2f2-9d9ee9dc2fe4?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27spec.json&response-content-type=application%2Fjson&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T102453Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=47ef39efb16aca25558c6bbdf9494df2bc805c5f1a126d33bb4bddf4fa9ab54b [following]\n",
            "--2024-02-05 10:24:53--  https://s3.tik.uni-stuttgart.de/fokus-dv-prod-2/10.18419/darus-2854/1811417c2f2-9d9ee9dc2fe4?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27spec.json&response-content-type=application%2Fjson&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240205T102453Z&X-Amz-SignedHeaders=host&X-Amz-Expires=172800&X-Amz-Credential=ZYFB5FYVYI021REQFP7K%2F20240205%2Fdataverse%2Fs3%2Faws4_request&X-Amz-Signature=47ef39efb16aca25558c6bbdf9494df2bc805c5f1a126d33bb4bddf4fa9ab54b\n",
            "Resolving s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)... 129.69.5.99, 129.69.5.100, 2001:7c0:2041:1db::99, ...\n",
            "Connecting to s3.tik.uni-stuttgart.de (s3.tik.uni-stuttgart.de)|129.69.5.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3736 (3.6K) [application/json]\n",
            "Saving to: ‘dataset/spec.json’\n",
            "\n",
            "spec.json           100%[===================>]   3.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-05 10:24:54 (1.21 GB/s) - ‘dataset/spec.json’ saved [3736/3736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset\n",
        "!wget -nc --content-disposition https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/8 -P dataset # dichasus-cf02\n",
        "!wget -nc --content-disposition https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/9 -P dataset # dichasus-cf03\n",
        "!wget -nc --content-disposition https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/10 -P dataset # dichasus-cf04\n",
        "!wget -nc https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf02.json -P dataset\n",
        "!wget -nc https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf03.json -P dataset\n",
        "!wget -nc https://dichasus.inue.uni-stuttgart.de/datasets/data/dichasus-cf0x/reftx-offsets-dichasus-cf04.json -P dataset\n",
        "!wget -nc --content-disposition https://darus.uni-stuttgart.de/api/access/datafile/:persistentId?persistentId=doi:10.18419/darus-2854/1 -P dataset # spec.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h-9ahSCtABXd",
        "outputId": "1d08fc29-eba7-4764-8940-a7c86ed5b558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sGPDwgiZ_98M"
      },
      "outputs": [],
      "source": [
        "from dichasus_cf0x import training_set\n",
        "import multiprocessing as mp\n",
        "import scipy.sparse.csgraph\n",
        "import sklearn.neighbors\n",
        "import tensorflow as tf\n",
        "import scipy.spatial\n",
        "import numpy as np\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def csi_time_domain(csi, pos, time):\n",
        "    csi = tf.signal.fftshift(tf.signal.ifft(tf.signal.fftshift(csi, axes = -1)), axes = -1)\n",
        "    return csi, pos, time\n",
        "\n",
        "def cut_out_taps(tap_start, tap_stop):\n",
        "    def cut_out_taps_func(csi, pos, time):\n",
        "        return csi[:,:,:,tap_start:tap_stop], pos, time\n",
        "\n",
        "    return cut_out_taps_func\n",
        "\n",
        "\n",
        "training_set = training_set.map(csi_time_domain, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "training_set = training_set.map(cut_out_taps(507, 520), num_parallel_calls = tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Nh5vyfN3B1Un"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classical_estimated_positions = np.load(\"results/estimated_positions.npy\")"
      ],
      "metadata": {
        "id": "b7KNoIExCAEJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth_positions = []\n",
        "csi_time_domain = []\n",
        "timestamps = []\n",
        "\n",
        "for csi, pos, time in training_set.prefetch(tf.data.AUTOTUNE).batch(1000):\n",
        "    csi_time_domain.append(csi.numpy())\n",
        "    groundtruth_positions.append(pos.numpy())\n",
        "    timestamps.append(time.numpy())\n",
        "\n",
        "csi_time_domain = np.concatenate(csi_time_domain)\n",
        "groundtruth_positions = np.concatenate(groundtruth_positions)\n",
        "timestamps = np.concatenate(timestamps)"
      ],
      "metadata": {
        "id": "PMcN7IfGCON9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adp_dissimilarity_matrix = np.zeros((csi_time_domain.shape[0], csi_time_domain.shape[0]), dtype=np.float32)\n",
        "\n",
        "def adp_dissimilarities_worker(todo_queue, output_queue):\n",
        "    def adp_dissimilarities(index):\n",
        "        # h has shape (arrays, antenna rows, antenna columns, taps), w has shape (datapoints, arrays, antenna rows, antenna columns, taps)\n",
        "        h = csi_time_domain[index]\n",
        "        w = csi_time_domain[index:]\n",
        "\n",
        "        dotproducts = np.abs(np.einsum(\"brmt,lbrmt->lbt\", np.conj(h), w))**2\n",
        "        norms = np.real(np.einsum(\"brmt,brmt->bt\", h, np.conj(h)) * np.einsum(\"lbrmt,lbrmt->lbt\", w, np.conj(w)))\n",
        "\n",
        "        return np.sum(1 - dotproducts / norms, axis = (1, 2))\n",
        "\n",
        "    while True:\n",
        "        index = todo_queue.get()\n",
        "\n",
        "        if index == -1:\n",
        "            output_queue.put((-1, None))\n",
        "            break\n",
        "\n",
        "        output_queue.put((index, adp_dissimilarities(index)))\n",
        "\n",
        "with tqdm.tqdm(total = csi_time_domain.shape[0]**2) as bar:\n",
        "    todo_queue = mp.Queue()\n",
        "    output_queue = mp.Queue()\n",
        "\n",
        "    for i in range(csi_time_domain.shape[0]):\n",
        "        todo_queue.put(i)\n",
        "\n",
        "    for i in range(mp.cpu_count()):\n",
        "        todo_queue.put(-1)\n",
        "        p = mp.Process(target = adp_dissimilarities_worker, args = (todo_queue, output_queue))\n",
        "        p.start()\n",
        "\n",
        "    finished_processes = 0\n",
        "    while finished_processes != mp.cpu_count():\n",
        "        i, d = output_queue.get()\n",
        "\n",
        "        if i == -1:\n",
        "            finished_processes = finished_processes + 1\n",
        "        else:\n",
        "            adp_dissimilarity_matrix[i,i:] = d\n",
        "            adp_dissimilarity_matrix[i:,i] = d\n",
        "            bar.update(2 * len(d) - 1)"
      ],
      "metadata": {
        "id": "Lhdca-hVC1-v",
        "outputId": "a2b682a9-4dea-4b57-ddfe-fd214779038e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/439866729 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 439866729/439866729 [20:04<00:00, 365173.96it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute timestamp-based dissimilarity matrix\n",
        "timestamp_dissimilarity_matrix = np.abs(np.subtract.outer(timestamps, timestamps))"
      ],
      "metadata": {
        "id": "rD9WLLQiGgpn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TIME_THRESHOLD = 2\n",
        "small_time_dissimilarity_indices = np.logical_and(timestamp_dissimilarity_matrix < TIME_THRESHOLD, timestamp_dissimilarity_matrix > 0)\n",
        "small_time_dissimilarities = timestamp_dissimilarity_matrix[small_time_dissimilarity_indices]\n",
        "small_adp_dissimilarities = adp_dissimilarity_matrix[small_time_dissimilarity_indices]\n",
        "\n",
        "occurences, edges = np.histogram(small_adp_dissimilarities / small_time_dissimilarities, range = (0, 50), bins = 1500)"
      ],
      "metadata": {
        "id": "6l-xNMXpGjY8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_centers = edges[:-1] + np.diff(edges) / 2.\n",
        "max_bin = np.argmax(occurences)\n",
        "min_threshold = np.quantile(occurences[:max_bin], 0.5)\n",
        "\n",
        "for threshold_bin in range(max_bin - 1, -1, -1):\n",
        "    if occurences[threshold_bin] < min_threshold:\n",
        "        break\n",
        "\n",
        "scaling_factor = bin_centers[threshold_bin]"
      ],
      "metadata": {
        "id": "hcjSw77RGmy1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse ADP-based and time-based dissimilarity matrices\n",
        "dissimilarity_matrix_fused = np.minimum(adp_dissimilarity_matrix, timestamp_dissimilarity_matrix * scaling_factor)"
      ],
      "metadata": {
        "id": "nR5bS2zCGpjJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_neighbors = 20\n",
        "\n",
        "nbrs_alg = sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors, metric=\"precomputed\", n_jobs = -1)\n",
        "nbrs = nbrs_alg.fit(dissimilarity_matrix_fused)\n",
        "nbg = sklearn.neighbors.kneighbors_graph(nbrs, n_neighbors, metric = \"precomputed\", mode=\"distance\")"
      ],
      "metadata": {
        "id": "AZHu5hN8GroV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dissimilarity_matrix_geodesic = np.zeros((nbg.shape[0], nbg.shape[1]), dtype = np.float32)\n",
        "\n",
        "def shortest_path_worker(todo_queue, output_queue):\n",
        "    while True:\n",
        "        index = todo_queue.get()\n",
        "\n",
        "        if index == -1:\n",
        "            output_queue.put((-1, None))\n",
        "            break\n",
        "\n",
        "        d = scipy.sparse.csgraph.dijkstra(nbg, directed=False, indices=index)\n",
        "        output_queue.put((index, d))\n",
        "\n",
        "with tqdm.tqdm(total = nbg.shape[0]**2) as bar:\n",
        "    todo_queue = mp.Queue()\n",
        "    output_queue = mp.Queue()\n",
        "\n",
        "    for i in range(nbg.shape[0]):\n",
        "        todo_queue.put(i)\n",
        "\n",
        "    for i in range(mp.cpu_count()):\n",
        "        todo_queue.put(-1)\n",
        "        p = mp.Process(target = shortest_path_worker, args = (todo_queue, output_queue))\n",
        "        p.start()\n",
        "\n",
        "    finished_processes = 0\n",
        "    while finished_processes != mp.cpu_count():\n",
        "        i, d = output_queue.get()\n",
        "\n",
        "        if i == -1:\n",
        "            finished_processes = finished_processes + 1\n",
        "        else:\n",
        "            dissimilarity_matrix_geodesic[i,:] = d\n",
        "            bar.update(len(d))"
      ],
      "metadata": {
        "id": "WGQu9bbnGuOE",
        "outputId": "5810fd3d-73de-4cc0-cd56-0b65d04f25c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/439866729 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 439866729/439866729 [08:15<00:00, 886945.84it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaling_nth_reduction = 30\n",
        "classical_positions_reduced = classical_estimated_positions[::scaling_nth_reduction]\n",
        "dissimilarity_matrix_reduced = dissimilarity_matrix_geodesic[::scaling_nth_reduction, ::scaling_nth_reduction]\n",
        "classical_distance_matrix = scipy.spatial.distance_matrix(classical_positions_reduced, classical_positions_reduced)"
      ],
      "metadata": {
        "id": "Ewa_LEUXGw_o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dissimilarity_unit_meters = np.full_like(dissimilarity_matrix_reduced, np.nan)\n",
        "diff = np.divide(dissimilarity_matrix_reduced, classical_distance_matrix, out = dissimilarity_unit_meters, where = classical_distance_matrix != 0)\n",
        "dissimilarity_unit_meters = dissimilarity_unit_meters.flatten()\n",
        "scaling_factor_meters = np.median(dissimilarity_unit_meters[np.isfinite(dissimilarity_unit_meters)])"
      ],
      "metadata": {
        "id": "Xgnbny4BGz4i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dissimilarity_matrix_geodesic_meters = dissimilarity_matrix_geodesic / scaling_factor_meters"
      ],
      "metadata": {
        "id": "1ZXYL4Z3G10I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"results/dissimilarity_matrix_geodesic_meters.npy\", dissimilarity_matrix_geodesic_meters)"
      ],
      "metadata": {
        "id": "xHS8Ck-bG35r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dichasus_cf0x import training_set, test_set, spec\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "slM5rinbJqQg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csi_time_domain(csi, pos, time):\n",
        "    csi = tf.signal.fftshift(tf.signal.ifft(tf.signal.fftshift(csi, axes = -1)), axes = -1)\n",
        "    return csi, pos, time\n",
        "\n",
        "def cut_out_taps(tap_start, tap_stop):\n",
        "    def cut_out_taps_func(csi, pos, time):\n",
        "        return csi[:,:,:,tap_start:tap_stop], pos, time\n",
        "\n",
        "    return cut_out_taps_func\n",
        "\n",
        "\n",
        "training_set = training_set.map(csi_time_domain, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "training_set = training_set.map(cut_out_taps(507, 520), num_parallel_calls = tf.data.AUTOTUNE)\n",
        "\n",
        "test_set = test_set.map(csi_time_domain, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "test_set = test_set.map(cut_out_taps(507, 520), num_parallel_calls = tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "nKVyRvjNJtz9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dissimilarity_matrix_geodesic_meters = np.load(\"results/dissimilarity_matrix_geodesic_meters.npy\")\n",
        "classical_estimated_positions = np.load(\"results/estimated_positions.npy\")\n",
        "estimated_aoas = np.load(\"results/estimated_aoas.npy\")\n",
        "estimated_toas = np.load(\"results/estimated_toas.npy\")\n",
        "delayspreads = np.load(\"results/delayspreads.npy\")"
      ],
      "metadata": {
        "id": "KPhh-WuSJwga"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth_positions = []\n",
        "csi_time_domain = []\n",
        "\n",
        "for csi, pos, time in training_set.prefetch(tf.data.AUTOTUNE).batch(1000):\n",
        "    csi_time_domain.append(csi.numpy())\n",
        "    groundtruth_positions.append(pos.numpy())\n",
        "\n",
        "csi_time_domain = np.concatenate(csi_time_domain)\n",
        "groundtruth_positions = np.concatenate(groundtruth_positions)"
      ],
      "metadata": {
        "id": "0zHGN9HxJ0J7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We assume that z-coordinate of TX is constant and known\n",
        "HEIGHT = np.mean(groundtruth_positions[:,2])"
      ],
      "metadata": {
        "id": "PaPG4tifJ299"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineeringLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureEngineeringLayer, self).__init__(dtype = tf.complex64)\n",
        "\n",
        "    def call(self, csi):\n",
        "        # Compute sample autocorrelations for any combination of two antennas in the whole system\n",
        "        # for the same datapoint and time tap.\n",
        "        # csi has shape (batchsize, array, antenna row, antenna column, tap)\n",
        "        sample_autocorrelations = tf.einsum(\"darmt,dbsnt->dtabrmsn\", csi, tf.math.conj(csi))\n",
        "        return tf.stack([tf.math.real(sample_autocorrelations), tf.math.imag(sample_autocorrelations)], axis = -1)"
      ],
      "metadata": {
        "id": "MeZTKNa1J44Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csi_time_domain_tensor = tf.constant(csi_time_domain)\n",
        "dissimilarity_matrix_geodesic_tensor = tf.constant(dissimilarity_matrix_geodesic_meters, dtype = tf.float32)\n",
        "estimated_aoas_tensor = tf.constant(estimated_aoas, dtype = tf.float32)\n",
        "estimated_toas_tensor = tf.constant(estimated_toas, dtype = tf.float32)\n",
        "delayspreads_tensor = tf.constant(delayspreads, dtype = tf.float32)"
      ],
      "metadata": {
        "id": "Cgx1ar2KJ7HY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapoint_count = tf.shape(csi_time_domain_tensor)[0].numpy()\n",
        "\n",
        "random_integer_pairs_dataset = tf.data.Dataset.zip(tf.data.Dataset.random(), tf.data.Dataset.random())\n",
        "\n",
        "@tf.function\n",
        "def fill_pairs(randA, randB):\n",
        "    indexA = randA % datapoint_count\n",
        "    indexB = randB % datapoint_count\n",
        "    input = (csi_time_domain_tensor[indexA], csi_time_domain_tensor[indexB])\n",
        "    labels = tf.concat([[dissimilarity_matrix_geodesic_tensor[indexA, indexB]], estimated_aoas_tensor[indexA], estimated_aoas_tensor[indexB], estimated_toas_tensor[indexA], estimated_toas_tensor[indexB], delayspreads_tensor[indexA], delayspreads_tensor[indexB]], 0)\n",
        "    return input, labels\n",
        "\n",
        "random_pair_dataset = random_integer_pairs_dataset.map(fill_pairs)"
      ],
      "metadata": {
        "id": "TXH8OA_MJ9TB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_count = np.shape(csi_time_domain)[1]\n",
        "rows_per_array_count = np.shape(csi_time_domain)[2]\n",
        "antennas_per_row_count = np.shape(csi_time_domain)[3]\n",
        "tap_count = np.shape(csi_time_domain)[4]\n",
        "\n",
        "cc_embmodel_input = tf.keras.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count), name=\"input\", dtype = tf.complex64)\n",
        "cc_embmodel_output = FeatureEngineeringLayer()(cc_embmodel_input)\n",
        "cc_embmodel_output = tf.keras.layers.Flatten()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(1024, activation = \"relu\")(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(512, activation = \"relu\")(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(256, activation = \"relu\")(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(128, activation = \"relu\")(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(64, activation = \"relu\")(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
        "cc_embmodel_output = tf.keras.layers.Dense(2, activation = \"linear\")(cc_embmodel_output)\n",
        "\n",
        "cc_embmodel = tf.keras.Model(inputs = cc_embmodel_input, outputs = cc_embmodel_output, name = \"ForwardChartingFunction\")"
      ],
      "metadata": {
        "id": "v1HmJys0J_Ua"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPEED_OF_LIGHT = 299792458\n",
        "BANDWIDTH = 50e6"
      ],
      "metadata": {
        "id": "dOvMUaITKCQI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are some empirically determined heuristics, may be further tweaked\n",
        "def get_aoa_vonmises_kappas(delayspreads):\n",
        "    return 0.0000003 / (delayspreads + 0.025e-7)\n",
        "\n",
        "def get_toa_variances(delayspreads):\n",
        "    return 1 + (delayspreads * 1e7) ** 4"
      ],
      "metadata": {
        "id": "8Rtd20fwKEwE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_positions = np.zeros((len(spec[\"antennas\"]), 3))\n",
        "array_normalvectors = np.zeros((len(spec[\"antennas\"]), 3))\n",
        "\n",
        "for antidx, antenna in enumerate(spec[\"antennas\"]):\n",
        "    array_positions[antidx] = np.asarray(antenna[\"location\"])\n",
        "    array_normalvectors[antidx] = np.asarray(antenna[\"direction\"])\n",
        "\n",
        "array_positions_tensor = tf.constant(array_positions, dtype = tf.float32)\n",
        "array_normalvectors_tensor = tf.constant(array_normalvectors, dtype = tf.float32)"
      ],
      "metadata": {
        "id": "4XEINivkKGsJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical AoA/TDoA-based likelihood function\n",
        "def classical_likelihood_func_vectorized(pos, aoas, toas, rms_delay_spreads):\n",
        "    pos_with_height = tf.concat([pos, HEIGHT * tf.ones(tf.shape(pos)[0])[:, tf.newaxis]], 1)\n",
        "\n",
        "    # \"relative\" has shape (number of positions, number of arrays, 3 spatial dimensions)\n",
        "    relative_pos = pos_with_height[:,tf.newaxis,:] - array_positions_tensor\n",
        "\n",
        "    # Compute ideal 2D AoAs at given positions, shape: (number of positions, number of arrays)\n",
        "    ideal_aoas = tf.math.atan2(-relative_pos[:, :, 1], -relative_pos[:, :, 0]) - tf.math.atan2(-array_normalvectors_tensor[:, 1], -array_normalvectors_tensor[:, 0])\n",
        "\n",
        "    # Compute ideal TDoAs at given positions, shape: (number of positions, number of arrays)\n",
        "    ideal_toas = tf.sqrt(tf.math.reduce_sum(tf.square(relative_pos), axis = -1)) / SPEED_OF_LIGHT * BANDWIDTH\n",
        "\n",
        "    # Compute AoA likelihoods based on von Mises distribution\n",
        "    kappa = get_aoa_vonmises_kappas(rms_delay_spreads)\n",
        "    aoa_likelihoods = tf.exp(kappa * tf.cos(ideal_aoas - aoas)) / (2 * np.pi * tf.math.bessel_i0(kappa))\n",
        "\n",
        "    # Compute TDoA likelihoods based on Gaussian distribution. tdoa_difference has shape (number of positions, number of array-pairs)\n",
        "    arraysA, arraysB = np.triu_indices(len(spec[\"antennas\"]), k = 1)\n",
        "    estimated_tdoas = tf.gather(toas, arraysB, axis = 1) - tf.gather(toas, arraysA, axis = 1)\n",
        "    ideal_tdoas = tf.gather(ideal_toas, arraysB, axis = 1) - tf.gather(ideal_toas, arraysA, axis = 1)\n",
        "    tdoa_difference = ideal_tdoas - estimated_tdoas\n",
        "    variances = get_toa_variances(tf.math.maximum(tf.gather(rms_delay_spreads, arraysA, axis = 1), tf.gather(rms_delay_spreads, arraysB, axis = 1)))\n",
        "    toa_likelihoods = tf.multiply(tf.divide(1, tf.sqrt(2 * np.pi * variances)), tf.exp(-0.5 * tf.divide(tf.square(tdoa_difference), variances)))\n",
        "\n",
        "    return tf.multiply(tf.math.reduce_prod(aoa_likelihoods, axis = -1), tf.math.reduce_prod(toa_likelihoods, axis = -1))"
      ],
      "metadata": {
        "id": "rvenFQ_SKKKH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_combined_siamese_classical_loss(classical_weight):\n",
        "    def combined_siamese_classical_loss(y_true, y_pred):\n",
        "        dissimilarities = y_true[:,0]\n",
        "\n",
        "        pos_A, pos_B = (y_pred[:,:2], y_pred[:,2:])\n",
        "        distances_pred = tf.math.reduce_euclidean_norm(pos_A - pos_B, axis = 1)\n",
        "        siamese_loss = tf.reduce_mean(tf.square(distances_pred - dissimilarities))\n",
        "\n",
        "        aoa_A = y_true[:,1:5]\n",
        "        aoa_B = y_true[:,5:9]\n",
        "        toa_A = y_true[:,9:13]\n",
        "        toa_B = y_true[:,13:17]\n",
        "        ds_A = y_true[:,17:21]\n",
        "        ds_B = y_true[:,21:25]\n",
        "        classical_loss = -tf.reduce_sum(classical_likelihood_func_vectorized(pos_A, aoa_A, toa_A, ds_A) + classical_likelihood_func_vectorized(pos_B, aoa_B, toa_B, ds_B))\n",
        "\n",
        "        return classical_weight * classical_loss + (1 - classical_weight) * siamese_loss\n",
        "\n",
        "    return combined_siamese_classical_loss"
      ],
      "metadata": {
        "id": "csJUvXE_KNeQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_A = tf.keras.layers.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count,), dtype = tf.complex64)\n",
        "input_B = tf.keras.layers.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count,), dtype = tf.complex64)\n",
        "\n",
        "embedding_A = cc_embmodel(input_A)\n",
        "embedding_B = cc_embmodel(input_B)\n",
        "\n",
        "output = tf.keras.layers.concatenate([embedding_A, embedding_B], axis=1)\n",
        "model = tf.keras.models.Model([input_A, input_B], output, name = \"SiameseNeuralNetwork\")"
      ],
      "metadata": {
        "id": "iuOR9ox3KQOi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Feel free to tweak these training hyperparameters - a good choice of these paremters is important for performance!\n",
        "samples_per_session = 200000\n",
        "learning_rates = [5e-3, 2e-3, 1e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
        "batch_sizes = [400, 800, 1200, 2000, 3000, 3000, 4000, 5000]\n",
        "classical_weights = [0.98, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]\n",
        "\n",
        "\n",
        "for session, learning_rate, batch_size, classical_weight in zip(range(len(learning_rates)), learning_rates, batch_sizes, classical_weights):\n",
        "    print(\"\\nTraining Session \", session + 1, \"\\nBatch Size: \", batch_size, \"\\nLearning rate: \", learning_rate)\n",
        "\n",
        "    # Fit model\n",
        "    model.compile(loss = get_combined_siamese_classical_loss(classical_weight), optimizer = optimizer)\n",
        "    optimizer.learning_rate.assign(learning_rate)\n",
        "    steps_per_epoch = int(samples_per_session / batch_size)\n",
        "    model.fit(random_pair_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE), steps_per_epoch = samples_per_session / batch_size)\n",
        "\n",
        "    # Quick evaluation\n",
        "    channel_chart_positions = cc_embmodel.predict(csi_time_domain_tensor)\n",
        "    errorvectors = groundtruth_positions[:,:2] - channel_chart_positions\n",
        "    print(f\"Mean Absolute Error (MAE): {np.mean(np.sqrt(errorvectors[:,0]**2 + errorvectors[:,1]**2)):.4f}m\")\n",
        "\n",
        "    #plot_colorized(channel_chart_positions, groundtruth_positions, title = \"Siamese-Based Channel Chart - Training Set\")\n",
        "    #plot_quiver(channel_chart_positions, groundtruth_positions)"
      ],
      "metadata": {
        "id": "jpR9RS8dKSF0",
        "outputId": "37ff3589-c93c-460d-9128-cc11bc68746a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Session  1 \n",
            "Batch Size:  400 \n",
            "Learning rate:  0.005\n",
            "500/500 [==============================] - 1410s 3s/step - loss: -0.1527\n",
            "656/656 [==============================] - 53s 79ms/step\n",
            "Mean Absolute Error (MAE): 0.4615m\n",
            "\n",
            "Training Session  2 \n",
            "Batch Size:  800 \n",
            "Learning rate:  0.002\n",
            " 17/250 [=>............................] - ETA: 19:39 - loss: -0.2757"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}